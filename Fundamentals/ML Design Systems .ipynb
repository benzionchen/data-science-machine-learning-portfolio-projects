{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16ba9d6",
   "metadata": {},
   "source": [
    "# Ensemble Learning Techniques\n",
    "\n",
    "Ensemble Learning is a ML paradigm where multiple models (often called \"learners\" or \"base models\") are generated and combined to solve a particular ML problem \n",
    "\n",
    "The idea is to build a prediction model by integrating outcomes of multiple smaller models together to improve robustness, accuracy, and performance. The goal is to reduce variance (bagging), bias (boosting) or improving predictions (stacking) \n",
    "\n",
    "Ensemble is a powerful approach in ML to achieve high accuracy and robustness across a wide range of tasks by combining multiple models. Ensembles can capture patterns in data that might be typically missed by individual models, making them popular for environments that require high predictive performance\n",
    "\n",
    "General Guidance:\n",
    "- Boosting is often chosen for problems where accuracy is paramount and the individual models are weak or biased.\n",
    "- Stacking is chosen when you have access to diverse models, and you're aiming for the best possible predictive performance, often at the expense of computational resources and model interpretability.\n",
    "- Bagging is preferred when dealing with high-variance models and when you want to improve robustness and stability without necessarily making the model more complex.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a53a4d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Bootstrap Aggregation (Bagging)\n",
    "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "Example: Random Forest Classifier\n",
    "\n",
    "### Process:\n",
    "Bagging involves training multiple models of the same type on different subsets of the training data. The subsets are created by randomly sampling with replacement from the original dataset (and some samples can appear more than once). The prediction is made by averaging the predictions (for regression problems) or by majority vote (for classification problems) \n",
    "\n",
    "Bagging is typically used in decision trees (Random Forests), but can be applied to most ML models to improve performance. \n",
    "\n",
    "\n",
    "### Sampling: \n",
    "You start with standard training dataset \"D\" with a sample size of \"n\". Bagging generates new training sets called \"m\" (each potentially varying in size). The size of \"m\" subsets are described by \"n'\"\n",
    "- \"With replacement\" means when you take the sample from the original dataset \"D\" and give it to \"m\", you don't remove that sample from the original dataset \"D\"\n",
    "\n",
    "The new training sets created through this process are the bootstramp samples. When n' = n (the new trianing sets are the same size as the original dataset D)\n",
    "- it's statistically expected that each bootstrap sample will contain about 63.2% unique instances from the original dataset (63.2% comes from the formula \"1 -(1/e)\" where \"e\" is the base of the natural logarithm, roughly = ~2.71828) \n",
    "\n",
    "Sampling with replacement ensures that the bootstrap sample is independent from others. The selection of one data point does not affect the selection of another so each training set can be considered independently created - this is crucial to ensure these models have diverse perspectives on data\n",
    "\n",
    "#### Why is Out-of-bag Dataset important? \n",
    "\n",
    "The original dataset (initial one) will have all the datapoints, but the bootstrapped ones might have repeats. And in the cases where there are repeated samples in the bootstrapped dataset, that would mean that some of the samples are not reflected, creating something called \"out-of-bag\" dataset where it's the remainder of samples not represented in the bootstrap dataset\n",
    "- `Original Dataset` (12 samples) minus `Bootstrap dataset` (7 unique samples, but 12 total samples) = `Out-of-bag Dataset` (5 samples) \n",
    "    - 12 - 7 = 5 \n",
    "    \n",
    "Out-of-bag dataset is used to test the accuracy of a random forest algorithm. For example, a model that produces 50 trees using the bootstrap and out-of-bag datasets will have a better accuracy than if it produced 10 trees. \n",
    "- the algorithm generates multiple trees, therefore, multiple datasets so the chance that an object is left ouf of the bootstrap dataset is low \n",
    "\n",
    "Decision trees are also just words for decision matrix\n",
    "\n",
    "\n",
    "#### Why is it ok to have duplicate values in the bootstrapped dataset? \n",
    "Multiple occurences in the same datapoint in the bootstrapped dataset but not in others will introduce variability, but when the predictions from these models are aggregated (through majority vote or averaged out), the ensemble can achieve more generalized peformance\n",
    "\n",
    "This ensemble model assumes that while each model may have its own biases due to its training dataset (like having duplicates of a certain datapoint), the aggregation process will even out the biases and lead to a final prediction that is more accurate and less prone to overfitting than any single model prediction\n",
    "\n",
    "\n",
    "### Model Training:\n",
    "Training - for each \"m\"-bootstrap samples, a separate model is trained on that data. This means you end up with \"m\"-number of models, each trained on a slightly different set of data due to random sampling process\n",
    "\n",
    "Combining - once we have the predictions from the models trained on \"m\", we combine the results\n",
    "- for regression: the output is averaged across all the \"m\"-models\n",
    "- for classification: there is a majority vote on what to classify \n",
    "\n",
    "\n",
    "### Pros:\n",
    "1. Bagging reduces variance (without increasing bias) leading to a model that generalizes better for unseen data \n",
    "    - however, bagging might not significantly reduce bias if a single model is already biased (but it also doesnt dramatically increase bias either)\n",
    "    - the main objective is variance reduction (when averaging outputs of multiple models, the variance decreases)\n",
    "2. Helps avoid overfitting when models get complex\n",
    "    - even though each subset model might have high-variance predictions due to overfitting to its bootstrapped dataset, averaging the models can cancel out the individual variances leading to a stable-er prediction\n",
    "\n",
    "\n",
    "### When to use Bagging\n",
    "- When you have a variance problem: Bagging is effective at reducing variance without increasing bias. If your model is overfitting the training data, bagging can help generalize better to unseen data\n",
    "- With stable and complex models: Bagging can be beneficial for complex models like deep learning or large decision trees that are prone to overfitting. It makes such models more robust by averaging out their predictions\n",
    "- For parallelizable training: Since each model in bagging can be trained independently of the others, bagging can be efficiently parallelized, making it suitable for problems where speed and efficiency are concerns\n",
    "- When model simplicity is not critical: Bagging, especially in the form of random forests, can lead to very accurate models, but these models can be large and not easily interpretable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5d474",
   "metadata": {},
   "source": [
    "## Boosting \n",
    "https://aws.amazon.com/what-is/boosting/\n",
    "\n",
    "Boosting focuses on reducing bias and building a strong model from a number of weak ones in sequential order. Each model in the sequence focuses on correctly predicting the instances that were misclassified by the previous model. Predictions from all models are then combined through a weighted vote (or sum) to produce a final prediction.\n",
    "- Example: a ML model that is trained to identify cats that was only trained on images of white cats might occasionally misidentify a black cat \n",
    "    - it's the wrong association with what defines a cat as a cat\n",
    "\n",
    "The technique builds the model in stages, and at each stage, it adjusts the weights of incorrectly classified instances so that subsequent models focus more on difficult cases. Boosting is particularly known for its ability to reduce bias and variance, leading to improved model accuracy. \n",
    "\n",
    "Boosting improves machine models' predictive accuracy and performance by converting multiple weak learners into a single strong learning model. Machine learning models can be weak or strong learners \n",
    "- weak have low prediction accuracy, similar to random guessing (prone to overfitting, cannot classify data that varies too much from original dataset)\n",
    "    - Ex: cats with pointed ears might not be identified as a cat according to the model if the cat has round ears\n",
    "- strong learners have higher prediction accuracy, (boostined turns a system of weak learners -> a single strong learner) \n",
    "\n",
    "Boosting creates an ensemble model by combining several weak decision trees sequentially. Similar to Bagging, Boosting is used to increase accuracy. Bagging improves accuracy of weak learners by training several of them at once on multiple datasets, in contrast, Boosting trains weak learners one after another (in a order).  \n",
    "\n",
    "Examples: \n",
    "- Adaptive Boost (AdaBoost) - adjust weights of incorrectly classified instances so that subsequent classifiers focus on difficult cases (this is one of the earlier boosting models, and tries to self-correct in every iteration of the boosting process)\n",
    "    - initially gives the same weight to each dataset and automatically adjusts the weights of the data points after every decision tree \n",
    "    - it gives more weight to incorrectly classified items to correct them for the next round, then repeats the process until the residual error, or the difference between actual and predicted values, falls below an acceptable threshold.\n",
    "    - can use this with many predictors, and it is typically not as sensitive as other boosting algorithms\n",
    "    - this approach does not work well when there is a correlation among features or high data dimensionality but overall, AdaBoost is a suitable type of boosting for classification problems\n",
    "\n",
    "- Gradient Boosting - builds models in sequential manner but uses the gradient of the loss function to guide the learning process (both for regression and classification \n",
    "    - similar to AdaBoost it is a sequential training technique and the difference between AdaBoost and GB is that GB does not give incorrectly classified items more weight. Instead, GB software optimizes the loss function by generating base learners sequentially so that the present base learner is always more effective than the previous one.\n",
    "\n",
    "- Extreme Gradient Boosting (XGBoost) - optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable (under the gradient boosting framework) \n",
    "    - XGBoost improves gradient boosting for computational speed and scale in several ways\n",
    "    - uses multiple cores on the CPU so that learning can occur in parallel during training and is a boosting algorithm that can handle extensive datasets, making it attractive for big data applications. The key features of XGBoost are parallelization, distributed computing, cache optimization, and out-of-core processing.\n",
    "\n",
    "\n",
    "### Process\n",
    "The process starts with a dataset and assigning equal weight to each instance of the dataset, and weights indicate the importance of correctly classifying each instance in the next model to be trained. \n",
    "\n",
    "Next, train a series of weak models (often decision trees) in a sequential manner. A \"weak model\" refers to a model that performs slightly better than random guessing but is still very simple with high bias. \n",
    "\n",
    "After training, adjust the weights of the instances based on the correctness of the model's predictions. Increase the weight of the instances taht were incorrectly predicted, making them more important and decrease the weights of the correctly predicted instances (less important for the next model). \n",
    "\n",
    "Then each model is given a weight that reflects its accuracy, and the final prediction is made by a wegihted vote (for classification) or weighted sum (for regression) of the predictions of all models. \n",
    "\n",
    "### Sampling\n",
    "Boosting does not involve sampling of instances to create multiple datasets (unlike bagging). Instead, all instances are used for each model, but their weights are adjusted, effectively changing their distribution for the next model. This method iteratively re-weights difficult predictions, making sure the ensemble pays more attention to them\n",
    "\n",
    "\n",
    "### Model Training\n",
    "Models are trained one at a time, with each model learning from the error of the previous one (sequential order is crucial because model performance determines which to emphasize moving forward). The individual models in boosting are often simple (weak learners) such as shallow decision trees. The rationale is that simple models contribute to overall model diversity and reduce risk of overfitting. Each model's influence on the final prediction is weighted by its accuracy. Models that perform better have more say in the final prediction than those with weaker performances\n",
    "\n",
    "Step 1. Assign equal value to each data sample, feed the data to the first ML model, and call it the base algorithm (base algorithm makes predictions for each data sample)\n",
    "\n",
    "Step 2. Boosting algorithm assesses model predictions and increases teh weight of samples with a more significant error, also assigning a weight based off of the model's performance. Models that output great predictions will have a high amount of infleunce of the final decision \n",
    "\n",
    "Step 3. Algorithm passes the weighted data to the next decision tree\n",
    "\n",
    "Step 4. Algorithm repeats step 2-3 until instances of training errors are below a certain threshold \n",
    "\n",
    "\n",
    "### When to use Boosting\n",
    "- When you have a bias problem: Model is too simple and underfitting the training data, boosting can help increase model complexity and reduce bias \n",
    "- With imbalanced data: Boosting has shown good performance on imbalanced datasets, especially AdaBoost, by focusing more on hard-to-classify instances\n",
    "- For improving accuracy: If your primary goal is to squeeze out every bit of accuracy from your model, and you're less concerned about model interpretability or computational efficiency, boosting is often a strong candidate\n",
    "- When individual models are weak: Boosting is designed to improve the performance of models that are slightly better than random guessing and effective when you're working with simple models and want to incrementally improve their performance\n",
    "\n",
    "\n",
    "### Cons \n",
    "Boosting is vulnerable to outlier data, or data values that are different than the rest of the dataset because each model attempts to correct the fualts of its predecessor, outliers can skew results significantly\n",
    "\n",
    "It also is challenging to use boosting for real-time implementation because the algorithm is more complex than other processes, boosting methods have high adaptability so you can use a wide variety of model parameters that immediately affect the model's performance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13264c74",
   "metadata": {},
   "source": [
    "\n",
    "## Stacked Generalization (Stacking/Blending) \n",
    "\n",
    "https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/\n",
    "\n",
    "Stacking involves training a new model to combine the predictions of several base models. Base models are trained on the complete training set, then their predictions are used as an input features for the final model (the \"stacker\" or \"meta-learner\") to make the final prediction. This approach can leverage the strength of each base model. The individual learners are weak and heterogeneous (while bagging and boosting individual learners are homogeneous)  \n",
    "\n",
    "The benefit of stacking is that it can harness the capabilities of a range of well-performing models on a classification or regression task and make predictions that have better performance than any single model in the ensemble.\n",
    "\n",
    "Addresses: Given multiple machine learning models that are skillful on a problem, but in different ways, how do you choose which model to use (trust)?\n",
    "- Unlike bagging, in stacking, the models are typically different (e.g. not all decision trees) and fit on the same dataset (e.g. instead of samples of the training dataset).\n",
    "- Unlike boosting, in stacking, a single model is used to learn how to best combine the predictions from the contributing models (e.g. instead of a sequence of models that correct the predictions of prior models).\n",
    "\n",
    "The architecture of a stacking model involves two or more base models, often referred to as level-0 models, and a meta-model that combines the predictions of the base models, referred to as a level-1 model.\n",
    "\n",
    "Level-0 Models (First level, Base-Models): Models fit on the training data and whose predictions are compiled.\n",
    "- these can include any mix of classifiers or regressors such as decision trees, NNs, or SVMs, etc. \n",
    "Level-1 Model (Second level, Meta-Model): Model that learns how to best combine the predictions of the base models.\n",
    "- trained on the outputs of the base model, where the input features are the predictions made by the base models on the validation set (meta-model learns how to best combine predictions to make a final prediction) \n",
    "- the meta-model in stacking learns to correct the predictions of the base model (if the base model are overfitting in different ways, the meta-model can leran to adjust for this by down-weighting or ignoring overly optimistic predictions) \n",
    "\n",
    "\n",
    "### Process\n",
    "\n",
    "https://youtu.be/xtTyEoJ31Vg?si=EVlZ-PR3rdh49ybq&t=407\n",
    "\n",
    "The meta-model is trained on the predictions made by base models on out-of-sample data. That is, data not used to train the base models is fed to the base models, predictions are made, and these predictions, along with the expected outputs, provide the input and output pairs of the training dataset used to fit the meta-model. The outputs from the base models used as input to the meta-model may be real value in the case of regression, and probability values, probability like values, or class labels in the case of classification. The most common approach to preparing the training dataset for the meta-model is via k-fold cross-validation of the base models, where the out-of-fold predictions are used as the basis for the training dataset for the meta-model.\n",
    "\n",
    "### Sampling\n",
    "Say there is a sample size of 100 data points. 75 of it is split off with 25 set to the side. \n",
    "\n",
    "Of the 75, 80% of it will be used to train different weak learner models like Support Vector Machine, Logistic Regression, Neural Network (these are called the base models). \n",
    "\n",
    "20% will be used for the test set. And then the results of these 20% will be used for the Meta-Model training. \n",
    "\n",
    "And the final model from the meta-model our final output becomes your final model and this final model is used to test the 25 sample size that was set aside previously to test the accuracy of this meta model.  \n",
    "\n",
    "Up until now, this is called \"Blending\"\n",
    "\n",
    "When we implement the k-fold approach to keep exchanging the data, this is called \"Stacking\"   \n",
    "\n",
    "\n",
    "### Model Training\n",
    "Though stacking can reduce overfitting, there still carries a risk of overfitting.\n",
    "\n",
    "The effectiveness depends on the choice of base models and the meta-model. If the meta-model is too complex relative to the amount of data available for training (particularly level-1 training data derived from the base model predictions), then it can overfit this data. If the meta-model is learning from the noise in the base models' predictions rather than the signal, it will perform poorly on unseen data. \n",
    "\n",
    "Proper training requires careful cross-validation strategies such as different folds of data for training the base models and generating predictions for the meta-model. Mistakes such as leadking data from the training to the validation phase will overfit. \n",
    "\n",
    "If base models are highly correlated (e.g. they are very similar models or overfit the training data in the same way) the stacking process will amplify the overfitting tendencies rather than mitigate it.\n",
    "\n",
    "\n",
    "### Pros\n",
    "Stacking combines multiple models to exploit their diverse strength and mitigate weaknesses, capturing different patterns in the data. \n",
    "\n",
    "By using using a meta-mode to learn how to optimally combine base model predictions, stacking can achieve better generalization performance (reducing risk of overfitting compared to individual models or simpler ensembles) \n",
    "\n",
    "Stacking is also highly customizable where practitioners choose from a wide range of models for both the base level and meta-level, tailoring the ensemble to their specific dataset and problem \n",
    "\n",
    "### When to use Stacking\n",
    "- When diversity is key: Stacking is effective when you can train diverse models that make different assumptions about the data. The diversity can come from using different types of models, different feature sets, or different hyperparameters\n",
    "- For maximizing predictive performance: Stacking can outperform individual models and other ensemble techniques by learning how to best combine their predictions. It's often used in machine learning competitions for this reason\n",
    "- When computational resources and time are not the primary concern: Stacking involves training multiple models and a meta-model, which can be computationally expensive and time-consuming, especially with large datasets and complex base models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab33d7f8",
   "metadata": {},
   "source": [
    "# Interpreting ML and Traditional ML Algorithms\n",
    "\n",
    "## Interpretability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6c5e7",
   "metadata": {},
   "source": [
    "# Sampling and Data Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ea6d9f",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "## Class-balanced Loss\n",
    "\n",
    "## Focal-loss \n",
    "\n",
    "## Cross-entropy loss\n",
    "\n",
    "## MSE loss\n",
    "\n",
    "## MAE loss\n",
    "\n",
    "## Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ed6ae",
   "metadata": {},
   "source": [
    "# Model and Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd368f0",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## L1 and I2 Regularization\n",
    "\n",
    "## Entropy Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ae6ce6",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63514e",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46c34fb",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "## AdaGrad \n",
    "\n",
    "## Momentum\n",
    "\n",
    "## RMSProp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b579c6",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "## ELU\n",
    "\n",
    "## ReLU\n",
    "\n",
    "## Tanh\n",
    "\n",
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef92fe0c",
   "metadata": {},
   "source": [
    "# Model Eval\n",
    "\n",
    "## FID Score\n",
    "\n",
    "## Inception score\n",
    "\n",
    "## BLEU metrics\n",
    "\n",
    "## METEOR metrics\n",
    "\n",
    "## ROUGE score\n",
    "\n",
    "## CIDEr score\n",
    "\n",
    "## SPICE score\n",
    "\n",
    "## Model Compression Survey\n",
    "\n",
    "## Shadow deployment\n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "## Canary Release \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f710c71d",
   "metadata": {},
   "source": [
    "# Quantization-aware training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018ba64",
   "metadata": {},
   "source": [
    "# Interleaving Experiment\n",
    "\n",
    "# Multi-armed Bandit\n",
    "\n",
    "# ML Infrastructure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
