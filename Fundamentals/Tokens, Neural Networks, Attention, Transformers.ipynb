{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8de4af5b",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8fb709",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "\"Token\" refers to an individual unit of data or text, and it's the outcome of the process of breaking down text into smaller parts that can be easily anaylzed or processed\n",
    "\n",
    "Tokens are building blocks for further analysis and model training in various NLP tasks. In context of Robotic Learning, it doesn't have a standardized universally accepted meaning like in NLP, but it can still symbolize information. \n",
    "\n",
    "## Symbolic Representation\n",
    "In symbolic AI (foundation for some approaches to robotics), a \"token\" might represent a discrete, symbolic pieceo of information, which could be used in the decision-making process (where different tokens represent different states, actions, or objects that a robot can recognize). Tokens are abstract representations of real-world entities or actions that a robot needs to understand\n",
    "\n",
    "For robotic manipulation tasks, \"tokens\" refer to a class of objects they interact with. For instance, in a sorting task, different types of objects might be assigned tokens that represent their category, and the robot's learning algorithm uses these tokens to make decisions about how to handle each ojbect. \n",
    "\n",
    "Tokens can also be referred to as instructions, or commands for the robot. For example, when teaching a robot to perform tasks through demonstration or verbal command. \n",
    "\n",
    "Tokens in reinforcement learning represent an element of state or action space. For example, when a robot is learning to navigate an environment, tokens could represent possible movements or interactions (e.g. move forward, turn left, etc.) or features of the environment that are relevant to the robot's learning process. \n",
    "\n",
    "In hierarchical learning systems, tokens might be used to represent higher-level goals or subtasks that a robot needs to accomplish as a part of a broader task, help in structuring the learnign process and decomposing complex tasks into manageable subtasks. \n",
    "\n",
    "## NLP \n",
    "In NLP, words, subwords, characters, phrases, or N-grams are cosnidered tokens. \n",
    "- For words, \"Machine Learning is fascinating\" can be tokenized as [\"Machine\", \"learning\", \"is\", \"fascinating\"]. \n",
    "- For subwords or characters, tokenization may help with handling unknown words, morphological varations, or languages with rich compounding \n",
    "- For phrases or N-grams, tokens can be consecutive sequences of words knowns n-grams\n",
    "    - for example, in bigram (2-gram) tokenization, the phrase \"natural language processing\" might be split into [\"natural language\", \"language processing\"]. \n",
    "    \n",
    "Tokenization is the process of converting text -> tokens (and crucial for preparing data for ML models from raw text into format algorithms understand/process). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daca2d9",
   "metadata": {},
   "source": [
    "# Attention Mechanism\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0665af",
   "metadata": {},
   "source": [
    "# Neural Networks\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
