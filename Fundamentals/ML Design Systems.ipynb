{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60fb5190",
   "metadata": {},
   "source": [
    "# Ensemble Learning Techniques\n",
    "\n",
    "Ensemble Learning is a ML paradigm where multiple models (often called \"learners\" or \"base models\") are generated and combined to solve a particular ML problem \n",
    "\n",
    "The idea is to build a prediction model by integrating outcomes of multiple smaller models together to improve robustness, accuracy, and performance. The goal is to reduce variance (bagging), bias (boosting) or improving predictions (stacking) \n",
    "\n",
    "Ensemble is a powerful approach in ML to achieve high accuracy and robustness across a wide range of tasks by combining multiple models. Ensembles can capture patterns in data that might be typically missed by individual models, making them popular for environments that require high predictive performance\n",
    "\n",
    "General Guidance:\n",
    "- Boosting is often chosen for problems where accuracy is paramount and the individual models are weak or biased.\n",
    "- Stacking is chosen when you have access to diverse models, and you're aiming for the best possible predictive performance, often at the expense of computational resources and model interpretability.\n",
    "- Bagging is preferred when dealing with high-variance models and when you want to improve robustness and stability without necessarily making the model more complex.\n",
    "\n",
    "\n",
    "\n",
    "## Bootstrap Aggregation (Bagging)\n",
    "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "Example: Random Forest Classifier\n",
    "\n",
    "### Process:\n",
    "Bagging involves training multiple models of the same type on different subsets of the training data. The subsets are created by randomly sampling with replacement from the original dataset (and some samples can appear more than once). The prediction is made by averaging the predictions (for regression problems) or by majority vote (for classification problems) \n",
    "\n",
    "Bagging is typically used in decision trees (Random Forests), but can be applied to most ML models to improve performance. \n",
    "\n",
    "### Sampling: \n",
    "You start with standard training dataset \"D\" with a sample size of \"n\". Bagging generates new training sets called \"m\" (each potentially varying in size). The size of \"m\" subsets are described by \"n'\"\n",
    "- \"With replacement\" means when you take the sample from the original dataset \"D\" and give it to \"m\", you don't remove that sample from the original dataset \"D\"\n",
    "\n",
    "The new training sets created through this process are the bootstramp samples. When n' = n (the new trianing sets are the same size as the original dataset D)\n",
    "- it's statistically expected that each bootstrap sample will contain about 63.2% unique instances from the original dataset (63.2% comes from the formula \"1 -(1/e)\" where \"e\" is the base of the natural logarithm, roughly = ~2.71828) \n",
    "\n",
    "Sampling with replacement ensures that the bootstrap sample is independent from others. The selection of one data point does not affect the selection of another so each training set can be considered independently created - this is crucial to ensure these models have diverse perspectives on data\n",
    "\n",
    "#### Why is it ok to have duplicate values in the bootstrapped dataset? \n",
    "Multiple occurences in the same datapoint in the bootstrapped dataset but not in others will introduce variability, but when the predictions from these models are aggregated (through majority vote or averaged out), the ensemble can achieve more generalized peformance\n",
    "\n",
    "This ensemble model assumes that while each model may have its own biases due to its training dataset (like having duplicates of a certain datapoint), the aggregation process will even out the biases and lead to a final prediction that is more accurate and less prone to overfitting than any single model prediction\n",
    "\n",
    "### Model Training:\n",
    "Training - for each \"m\"-bootstrap samples, a separate model is trained on that data. This means you end up with \"m\"-number of models, each trained on a slightly different set of data due to random sampling process\n",
    "\n",
    "Combining - once we have the predictions from the models trained on \"m\", we combine the results\n",
    "- for regression: the output is averaged across all the \"m\"-models\n",
    "- for classification: there is a majority vote on what to classify \n",
    "\n",
    "### Pros:\n",
    "1. Bagging reduces variance (without increasing bias) leading to a model that generalizes better for unseen data \n",
    "    - however, bagging might not significantly reduce bias if a single model is already biased (but it also doesnt dramatically increase bias either)\n",
    "    - the main objective is variance reduction (when averaging outputs of multiple models, the variance decreases)\n",
    "2. Helps avoid overfitting when models get complex\n",
    "    - even though each subset model might have high-variance predictions due to overfitting to its bootstrapped dataset, averaging the models can cancel out the individual variances leading to a stable-er prediction\n",
    "\n",
    "### When to use Bagging\n",
    "- When you have a variance problem: Bagging is effective at reducing variance without increasing bias. If your model is overfitting the training data, bagging can help generalize better to unseen data\n",
    "- With stable and complex models: Bagging can be beneficial for complex models like deep learning or large decision trees that are prone to overfitting. It makes such models more robust by averaging out their predictions\n",
    "- For parallelizable training: Since each model in bagging can be trained independently of the others, bagging can be efficiently parallelized, making it suitable for problems where speed and efficiency are concerns\n",
    "- When model simplicity is not critical: Bagging, especially in the form of random forests, can lead to very accurate models, but these models can be large and not easily interpretable\n",
    "\n",
    "\n",
    "## Boosting \n",
    "Boosting focuses on reducing bias and building a strong model from a number of weak ones in sequential order. Each model in the sequence focuses on correctly predicting the instances that were misclassified by the previous model. Predictions from all models are then combined through a weighted vote (or sum) to produce a final prediction.\n",
    "\n",
    "The technique builds the model in stages, and at each stage, it adjusts the weights of incorrectly classified instances so that subsequent models focus more on difficult cases. Boosting is particularly known for its ability to reduce bias and variance, leading to improved model accuracy. \n",
    "\n",
    "\n",
    "Examples: \n",
    "- Adaptive Boost (AdaBoost) - adjust weights of incorrectly classified instances so that subsequent classifiers focus on difficult cases\n",
    "- Gradient Boosting - builds models in sequential manner but uses the gradient of the loss function to guide the learning process (both for regression and classification \n",
    "- Extreme Gradient Boosting (XGBoost) - optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable (under the gradient boosting framework) \n",
    "\n",
    "\n",
    "### Process\n",
    "The process starts with a dataset and assigning equal weight to each instance of the dataset, and weights indicate the importance of correctly classifying each instance in the next model to be trained. \n",
    "\n",
    "Next, train a series of weak models (often decision trees) in a sequential manner. A \"weak model\" refers to a model that performs slightly better than random guessing but is still very simple with high bias. \n",
    "\n",
    "After training, adjust the weights of the instances based on the correctness of the model's predictions. Increase the weight of the instances taht were incorrectly predicted, making them more important and decrease the weights of the correctly predicted instances (less important for the next model). \n",
    "\n",
    "Then each model is given a weight that reflects its accuracy, and the final prediction is made by a wegihted vote (for classification) or weighted sum (for regression) of the predictions of all models. \n",
    "\n",
    "### Sampling\n",
    "\n",
    "Boosting does not involve sampling of instances to create multiple datasets (unlike bagging). Instead, all instances are used for each model, but their weights are adjusted, effectively changing their distribution for the next model. This method iteratively re-weights difficult predictions, making sure the ensemble pays more attention to them\n",
    "\n",
    "### Model Training\n",
    "Models are trained one at a time, with each model learning from the error of the previous one (sequential order is crucial because model performance determines which to emphasize moving forward) \n",
    "\n",
    "The individual models n boosting are often simple (weak learners) such as shallow decision trees. The rationale is that simple models contribute to overall model diversity and reduce risk of overfitting. \n",
    "\n",
    "Each model's influence on the final prediction is weighted by its accuracy. Models that perform better have more say in the final prediction than those with weaker performances\n",
    "\n",
    "### When to use Boosting\n",
    "- When you have a bias problem: Model is too simple and underfitting the training data, boosting can help increase model complexity and reduce bias \n",
    "- With imbalanced data: Boosting has shown good performance on imbalanced datasets, especially AdaBoost, by focusing more on hard-to-classify instances\n",
    "- For improving accuracy: If your primary goal is to squeeze out every bit of accuracy from your model, and you're less concerned about model interpretability or computational efficiency, boosting is often a strong candidate\n",
    "- When individual models are weak: Boosting is designed to improve the performance of models that are slightly better than random guessing and effective when you're working with simple models and want to incrementally improve their performance\n",
    "\n",
    "\n",
    "\n",
    "## Stacked Generalization (Stacking) \n",
    "Stacking involves training a new model to combine the predictions of several base models. Base models are trained on the complete training set, then their predictions are used as an input features for the final model (the \"stacker\" or \"meta-learner\") to make the final prediction. This approach can leverage the strength of each base model. \n",
    "\n",
    "### Process\n",
    "\n",
    "### Sampling\n",
    "\n",
    "### Model Training\n",
    "\n",
    "### Pros\n",
    "\n",
    "### When to use Stacking\n",
    "- When diversity is key: Stacking is effective when you can train diverse models that make different assumptions about the data. The diversity can come from using different types of models, different feature sets, or different hyperparameters\n",
    "- For maximizing predictive performance: Stacking can outperform individual models and other ensemble techniques by learning how to best combine their predictions. It's often used in machine learning competitions for this reason\n",
    "- When computational resources and time are not the primary concern: Stacking involves training multiple models and a meta-model, which can be computationally expensive and time-consuming, especially with large datasets and complex base models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77d513",
   "metadata": {},
   "source": [
    "# Interpreting ML and Traditional ML Algorithms\n",
    "\n",
    "## Interpretability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c237671a",
   "metadata": {},
   "source": [
    "# Sampling and Data Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11105c9",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "## Class-balanced Loss\n",
    "\n",
    "## Focal-loss \n",
    "\n",
    "## Cross-entropy loss\n",
    "\n",
    "## MSE loss\n",
    "\n",
    "## MAE loss\n",
    "\n",
    "## Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c9e443",
   "metadata": {},
   "source": [
    "# Model and Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ce610",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## L1 and I2 Regularization\n",
    "\n",
    "## Entropy Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125276b9",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e0dd3",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a7faa",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "## AdaGrad \n",
    "\n",
    "## Momentum\n",
    "\n",
    "## RMSProp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0600a9e1",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "## ELU\n",
    "\n",
    "## ReLU\n",
    "\n",
    "## Tanh\n",
    "\n",
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42669ea",
   "metadata": {},
   "source": [
    "# Model Eval\n",
    "\n",
    "## FID Score\n",
    "\n",
    "## Inception score\n",
    "\n",
    "## BLEU metrics\n",
    "\n",
    "## METEOR metrics\n",
    "\n",
    "## ROUGE score\n",
    "\n",
    "## CIDEr score\n",
    "\n",
    "## SPICE score\n",
    "\n",
    "## Model Compression Survey\n",
    "\n",
    "## Shadow deployment\n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "## Canary Release \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee4844",
   "metadata": {},
   "source": [
    "# Quantization-aware training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251bc086",
   "metadata": {},
   "source": [
    "# Interleaving Experiment\n",
    "\n",
    "# Multi-armed Bandit\n",
    "\n",
    "# ML Infrastructure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
