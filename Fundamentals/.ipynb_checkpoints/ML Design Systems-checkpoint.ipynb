{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88e75e43",
   "metadata": {},
   "source": [
    "# Ensemble Learning Techniques\n",
    "\n",
    "Ensemble Learning is a ML paradigm where multiple models (often called \"learners\" or \"base models\") are generated and combined to solve a particular ML problem. \n",
    "\n",
    "The idea is to build a prediction model by integrating outcomes of multiple smaller models together to improve robustness, accuracy, and performance. The goal is to reduce variance (bagging), bias (boosting) or improving predictions (stacking) \n",
    "\n",
    "\n",
    "## Bootstrap Aggregation (Bagging)\n",
    "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "### Process:\n",
    "Bagging involves training multiple models of the same type on different subsets of the training data. The subsets are created by randomly sampling with replacement from the original dataset (and some samples can appear more than once). The prediction is made by averaging the predictions (for regression problems) or by majority vote (for classification problems) \n",
    "\n",
    "Bagging is typically used in decision trees (Random Forests), but can be applied to most ML models to improve performance. \n",
    "\n",
    "### Sampling: \n",
    "You start with standard training dataset \"D\" with a sample size of \"n\". Bagging generates new training sets called \"m\" (each potentially varying in size). The size of \"m\" subsets are described by \"n'\"\n",
    "- \"With replacement\" means when you take the sample from the original dataset \"D\" and give it to \"m\", you don't remove that sample from the original dataset \"D\"\n",
    "\n",
    "The new training sets created through this process are the bootstramp samples. When n' = n (the new trianing sets are the same size as the original dataset D)\n",
    "- it's statistically expected that each bootstrap sample will contain about 63.2% unique instances from the original dataset (63.2% comes from the formula \"1 -(1/e)\" where \"e\" is the base of the natural logarithm, roughly = ~2.71828) \n",
    "\n",
    "Sampling with replacement ensures that the bootstrap sample is independent from others. The selection of one data point does not affect the selection of another so each training set can be considered independently created - this is crucial to ensure these models have diverse perspectives on data\n",
    "\n",
    "#### Why is it ok to have duplicate values in the bootstrapped dataset? \n",
    "Multiple occurences in the same datapoint in the bootstrapped dataset but not in others will introduce variability, but when the predictions from these models are aggregated (through majority vote or averaged out), the ensemble can achieve more generalized peformance\n",
    "\n",
    "This ensemble model assumes that while each model may have its own biases due to its training dataset (like having duplicates of a certain datapoint), the aggregation process will even out the biases and lead to a final prediction that is more accurate and less prone to overfitting than any single model prediction\n",
    "\n",
    "### Model Training:\n",
    "Training - for each \"m\"-bootstrap samples, a separate model is trained on that data. This means you end up with \"m\"-number of models, each trained on a slightly different set of data due to random sampling process\n",
    "\n",
    "Combining - once we have the predictions from the models trained on \"m\", we combine the results\n",
    "- for regression: the output is averaged across all the \"m\"-models\n",
    "- for classification: there is a majority vote on what to classify \n",
    "\n",
    "### Pros:\n",
    "1. Bagging reduces variance (without increasing bias) leading to a model that generalizes better for unseen data \n",
    "    - however, bagging might not significantly reduce bias if a single model is already biased (but it also doesnt dramatically increase bias either)\n",
    "    - the main objective is variance reduction (when averaging outputs of multiple models, the variance decreases)\n",
    "2. Helps avoid overfitting when models get complex\n",
    "    - even though each subset model might have high-variance predictions due to overfitting to its bootstrapped dataset, averaging the models can cancel out the individual variances leading to a stable-er prediction\n",
    "\n",
    "\n",
    "## Boosting \n",
    "\n",
    "\n",
    "\n",
    "## Stacking "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f483add",
   "metadata": {},
   "source": [
    "# Interpreting ML and Traditional ML Algorithms\n",
    "\n",
    "## Interpretability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3268c1c4",
   "metadata": {},
   "source": [
    "# Sampling and Data Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508ded5",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "## Class-balanced Loss\n",
    "\n",
    "## Focal-loss \n",
    "\n",
    "## Cross-entropy loss\n",
    "\n",
    "## MSE loss\n",
    "\n",
    "## MAE loss\n",
    "\n",
    "## Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06769004",
   "metadata": {},
   "source": [
    "# Model and Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd801f3",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## L1 and I2 Regularization\n",
    "\n",
    "## Entropy Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453cb4c9",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595144f0",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78708f44",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "## AdaGrad \n",
    "\n",
    "## Momentum\n",
    "\n",
    "## RMSProp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5692ce1",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "## ELU\n",
    "\n",
    "## ReLU\n",
    "\n",
    "## Tanh\n",
    "\n",
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be30ead",
   "metadata": {},
   "source": [
    "# Model Eval\n",
    "\n",
    "## FID Score\n",
    "\n",
    "## Inception score\n",
    "\n",
    "## BLEU metrics\n",
    "\n",
    "## METEOR metrics\n",
    "\n",
    "## ROUGE score\n",
    "\n",
    "## CIDEr score\n",
    "\n",
    "## SPICE score\n",
    "\n",
    "## Model Compression Survey\n",
    "\n",
    "## Shadow deployment\n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "## Canary Release \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540fa02c",
   "metadata": {},
   "source": [
    "# Quantization-aware training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33336f1b",
   "metadata": {},
   "source": [
    "# Interleaving Experiment\n",
    "\n",
    "# Multi-armed Bandit\n",
    "\n",
    "# ML Infrastructure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
