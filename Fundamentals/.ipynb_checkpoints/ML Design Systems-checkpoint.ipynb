{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85a27dbb",
   "metadata": {},
   "source": [
    "# Ensemble Learning Techniques\n",
    "\n",
    "Ensemble Learning is a ML paradigm where multiple models (often called \"learners\" or \"base models\") are generated and combined to solve a particular ML problem \n",
    "\n",
    "The idea is to build a prediction model by integrating outcomes of multiple smaller models together to improve robustness, accuracy, and performance. The goal is to reduce variance (bagging), bias (boosting) or improving predictions (stacking) \n",
    "\n",
    "Ensemble is a powerful approach in ML to achieve high accuracy and robustness across a wide range of tasks by combining multiple models. Ensembles can capture patterns in data that might be typically missed by individual models, making them popular for environments that require high predictive performance\n",
    "\n",
    "\n",
    "## Bootstrap Aggregation (Bagging)\n",
    "https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "Example: Random Forest Classifier\n",
    "\n",
    "### Process:\n",
    "Bagging involves training multiple models of the same type on different subsets of the training data. The subsets are created by randomly sampling with replacement from the original dataset (and some samples can appear more than once). The prediction is made by averaging the predictions (for regression problems) or by majority vote (for classification problems) \n",
    "\n",
    "Bagging is typically used in decision trees (Random Forests), but can be applied to most ML models to improve performance. \n",
    "\n",
    "### Sampling: \n",
    "You start with standard training dataset \"D\" with a sample size of \"n\". Bagging generates new training sets called \"m\" (each potentially varying in size). The size of \"m\" subsets are described by \"n'\"\n",
    "- \"With replacement\" means when you take the sample from the original dataset \"D\" and give it to \"m\", you don't remove that sample from the original dataset \"D\"\n",
    "\n",
    "The new training sets created through this process are the bootstramp samples. When n' = n (the new trianing sets are the same size as the original dataset D)\n",
    "- it's statistically expected that each bootstrap sample will contain about 63.2% unique instances from the original dataset (63.2% comes from the formula \"1 -(1/e)\" where \"e\" is the base of the natural logarithm, roughly = ~2.71828) \n",
    "\n",
    "Sampling with replacement ensures that the bootstrap sample is independent from others. The selection of one data point does not affect the selection of another so each training set can be considered independently created - this is crucial to ensure these models have diverse perspectives on data\n",
    "\n",
    "#### Why is it ok to have duplicate values in the bootstrapped dataset? \n",
    "Multiple occurences in the same datapoint in the bootstrapped dataset but not in others will introduce variability, but when the predictions from these models are aggregated (through majority vote or averaged out), the ensemble can achieve more generalized peformance\n",
    "\n",
    "This ensemble model assumes that while each model may have its own biases due to its training dataset (like having duplicates of a certain datapoint), the aggregation process will even out the biases and lead to a final prediction that is more accurate and less prone to overfitting than any single model prediction\n",
    "\n",
    "### Model Training:\n",
    "Training - for each \"m\"-bootstrap samples, a separate model is trained on that data. This means you end up with \"m\"-number of models, each trained on a slightly different set of data due to random sampling process\n",
    "\n",
    "Combining - once we have the predictions from the models trained on \"m\", we combine the results\n",
    "- for regression: the output is averaged across all the \"m\"-models\n",
    "- for classification: there is a majority vote on what to classify \n",
    "\n",
    "### Pros:\n",
    "1. Bagging reduces variance (without increasing bias) leading to a model that generalizes better for unseen data \n",
    "    - however, bagging might not significantly reduce bias if a single model is already biased (but it also doesnt dramatically increase bias either)\n",
    "    - the main objective is variance reduction (when averaging outputs of multiple models, the variance decreases)\n",
    "2. Helps avoid overfitting when models get complex\n",
    "    - even though each subset model might have high-variance predictions due to overfitting to its bootstrapped dataset, averaging the models can cancel out the individual variances leading to a stable-er prediction\n",
    "\n",
    "\n",
    "\n",
    "## Boosting \n",
    "Boosting focuses on reducing bias and building a strong model from a number of weak ones in sequential order. Each model in the sequence focuses on correctly predicting the instances that were misclassified by the previous model. Predictions from all models are then combined through a weighted vote (or sum) to produce a final prediction.\n",
    "\n",
    "The technique builds the model in stages, and at each stage, it adjusts the weights of incorrectly classified instances so that subsequent models focus more on difficult cases. Boosting is particularly known for its ability to reduce bias and variance, leading to improved model accuracy. \n",
    "\n",
    "\n",
    "Examples: \n",
    "- Adaptive Boost (AdaBoost) - adjust weights of incorrectly classified instances so that subsequent classifiers focus on difficult cases\n",
    "- Gradient Boosting - builds models in sequential manner but uses the gradient of the loss function to guide the learning process (both for regression and classification \n",
    "- Extreme Gradient Boosting (XGBoost) - optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable (under the gradient boosting framework) \n",
    "\n",
    "\n",
    "### Process\n",
    "The process starts with a dataset and assigning equal weight to each instance of the dataset, and weights indicate the importance of correctly classifying each instance in the next model to be trained. \n",
    "\n",
    "Next, train a series of weak models (often decision trees) in a sequential manner. A \"weak model\" refers to a model that performs slightly better than random guessing but is still very simple with high bias. \n",
    "\n",
    "After training, adjust the weights of the instances based on the correctness of the model's predictions. Increase the weight of the instances taht were incorrectly predicted, making them more important and decrease the weights of the correctly predicted instances (less important for the next model). \n",
    "\n",
    "Then each model is given a weight that reflects its accuracy, and the final prediction is made by a wegihted vote (for classification) or weighted sum (for regression) of the predictions of all models. \n",
    "\n",
    "### Sampling\n",
    "\n",
    "Boosting does not involve sampling of instances to create multiple datasets (unlike bagging). Instead, all instances are used for each model, but their weights are adjusted, effectively changing their distribution for the next model. This method iteratively re-weights difficult predictions, making sure the ensemble pays more attention to them\n",
    "\n",
    "### Model Training\n",
    "Models are trained one at a time, with each model learning from the error of the previous one (sequential order is crucial because model performance determines which to emphasize moving forward) \n",
    "\n",
    "The individual models n boosting are often simple (weak learners) such as shallow decision trees. The rationale is that simple models contribute to overall model diversity and reduce risk of overfitting. \n",
    "\n",
    "Each model's influence on the final prediction is weighted by its accuracy. Models that perform better have more say in the final prediction than those with weaker performances\n",
    "\n",
    "\n",
    "## Stacked Generalization (Stacking) \n",
    "Stacking involves training a new model to combine the predictions of several base models. Base models are trained on the complete training set, then their predictions are used as an input features for the final model (the \"stacker\" or \"meta-learner\") to make the final prediction. This approach can leverage the strength of each base model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08443a34",
   "metadata": {},
   "source": [
    "# Interpreting ML and Traditional ML Algorithms\n",
    "\n",
    "## Interpretability Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ee99c6",
   "metadata": {},
   "source": [
    "# Sampling and Data Splitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01ce690",
   "metadata": {},
   "source": [
    "# Loss\n",
    "\n",
    "## Class-balanced Loss\n",
    "\n",
    "## Focal-loss \n",
    "\n",
    "## Cross-entropy loss\n",
    "\n",
    "## MSE loss\n",
    "\n",
    "## MAE loss\n",
    "\n",
    "## Huber loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae388e",
   "metadata": {},
   "source": [
    "# Model and Data Parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d40311",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "## L1 and I2 Regularization\n",
    "\n",
    "## Entropy Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f288e",
   "metadata": {},
   "source": [
    "# K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8176c9",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc43340",
   "metadata": {},
   "source": [
    "# Optimization Algorithms\n",
    "\n",
    "## Stochastic Gradient Descent\n",
    "\n",
    "## AdaGrad \n",
    "\n",
    "## Momentum\n",
    "\n",
    "## RMSProp \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00cbac",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "## ELU\n",
    "\n",
    "## ReLU\n",
    "\n",
    "## Tanh\n",
    "\n",
    "## Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a1957c",
   "metadata": {},
   "source": [
    "# Model Eval\n",
    "\n",
    "## FID Score\n",
    "\n",
    "## Inception score\n",
    "\n",
    "## BLEU metrics\n",
    "\n",
    "## METEOR metrics\n",
    "\n",
    "## ROUGE score\n",
    "\n",
    "## CIDEr score\n",
    "\n",
    "## SPICE score\n",
    "\n",
    "## Model Compression Survey\n",
    "\n",
    "## Shadow deployment\n",
    "\n",
    "## A/B Testing\n",
    "\n",
    "## Canary Release \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bb67ce",
   "metadata": {},
   "source": [
    "# Quantization-aware training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d0c41",
   "metadata": {},
   "source": [
    "# Interleaving Experiment\n",
    "\n",
    "# Multi-armed Bandit\n",
    "\n",
    "# ML Infrastructure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
